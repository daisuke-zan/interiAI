import asyncio
import io

import pandas as pd
import streamlit as st
import streamlit_antd_components as sac
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig
from sitemapparser import SiteMapParser
from sitemapparser.exporters import JSONExporter


def init():
    st.set_page_config(page_title="Interior AI Demo ",
                       page_icon="ğŸª‘",
                       layout="wide",
                       initial_sidebar_state="collapsed")
    # ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆ
    if 'scraping_data_source_type' not in st.session_state:
        st.session_state['scraping_data_source_type'] = 0
    if 'scraping_all_url_list' not in st.session_state:
        st.session_state['scraping_all_url_list'] = pd.DataFrame()
    if 'scraping_selected_url_list' not in st.session_state:
        st.session_state['scraping_selected_url_list'] = pd.DataFrame()


def sidebar():
    with st.sidebar:
        selected_menu = sac.menu([
            sac.MenuItem('å®¶å…·é¸å®š', icon='house-door-fill'),
            sac.MenuItem('å•†å“ãƒ‡ãƒ¼ã‚¿ä½œæˆ',
                         icon='database-fill',
                         children=[
                             sac.MenuItem('ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°', icon='globe'),
                             sac.MenuItem('PDFãƒ‡ãƒ¼ã‚¿æŠ½å‡º', icon='filetype-pdf')
                         ])
        ],
                                 open_all=True)
    return selected_menu


def search_conditions():
    with st.expander(label='é¸å®šæ¡ä»¶', expanded=True):
        # ãƒ†ã‚¤ã‚¹ãƒˆ
        with st.container(border=True):
            taste = sac.chip(items=[
                sac.ChipItem(label='ãƒŠãƒãƒ¥ãƒ©ãƒ«', icon='tree-fill'),
                sac.ChipItem(label='ã‚¤ãƒ³ãƒ€ã‚¹ãƒˆãƒªã‚¢ãƒ«', icon='building-fill'),
            ],
                             label='ãƒ†ã‚¤ã‚¹ãƒˆ',
                             align='start',
                             radius='xl',
                             variant='light',
                             multiple=False)

        # ç´æœŸ
        with st.container(border=True):
            delivery = sac.chip(items=[
                sac.ChipItem(label='åœ¨åº«å“', icon='truck-fill'),
                sac.ChipItem(label='3é€±é–“', icon='calendar-check-fill'),
                sac.ChipItem(label='1ãƒ¶æœˆ', icon='calendar-check-fill'),
                sac.ChipItem(label='1.5ã€œ2ãƒ¶æœˆ', icon='calendar-check-fill'),
            ],
                                label='ç´æœŸ',
                                align='start',
                                radius='xl',
                                variant='outline',
                                multiple=False)

        # ä¾¡æ ¼å¸¯
        with st.container(border=True):
            price = sac.chip(items=[
                sac.ChipItem(label='Low', icon='cash-coin'),
                sac.ChipItem(label='Middle', icon='cash-coin'),
                sac.ChipItem(label='High', icon='cash-coin')
            ],
                             label='ä¾¡æ ¼å¸¯',
                             align='start',
                             radius='xl',
                             variant='outline',
                             multiple=True)
        # ã‚«ãƒ†ã‚´ãƒª
        with st.container(border=True):
            category = sac.cascader(items=[
                sac.CasItem('ã‚½ãƒ•ã‚¡',
                            children=[
                                sac.CasItem('1äººæ›ã‘ã‚½ãƒ•ã‚¡'),
                                sac.CasItem('2äººæ›ã‘ã‚½ãƒ•ã‚¡'),
                                sac.CasItem('3äººæ›ã‘ã‚½ãƒ•ã‚¡'),
                            ]),
                sac.CasItem('ãƒã‚§ã‚¢ãƒ»æ¤…å­',
                            children=[
                                sac.CasItem('ã‚ªãƒ•ã‚£ã‚¹ãƒã‚§ã‚¢ãƒ»ãƒ¯ãƒ¼ã‚¯ãƒã‚§ã‚¢'),
                                sac.CasItem('ãƒŸãƒ¼ãƒ†ã‚£ãƒ³ã‚°ãƒã‚§ã‚¢'),
                                sac.CasItem('ãƒ€ã‚¤ãƒ‹ãƒ³ã‚°ãƒã‚§ã‚¢'),
                            ]),
            ],
                                    label='ã‚«ãƒ†ã‚´ãƒª',
                                    placeholder='ã‚«ãƒ†ã‚´ãƒªã‚’é¸æŠã—ã¦ãã ã•ã„ï¼ˆã¾ã å…¨ã‚«ãƒ†ã‚´ãƒªå…¥ã‚Œã¦ã¾ã›ã‚“ï¼‰',
                                    multiple=True,
                                    search=True,
                                    clear=True)
        # ã‚µã‚¤ã‚º
        with st.container(border=True):
            st.caption('ã‚µã‚¤ã‚º')
            col1, col2, col3, col4 = st.columns(spec=4, gap='small', border=True)
            with col1:
                # å¹…
                witdh = st.slider(label='å¹…',
                                  min_value=0,
                                  max_value=8000,
                                  value=(0, 8000),
                                  step=100,
                                  format='%d mm',
                                  help='å¹…ã®ç¯„å›²ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚')
            with col2:
                # å¥¥è¡Œ
                depth = st.slider(label='å¥¥è¡Œ',
                                  min_value=0,
                                  max_value=4000,
                                  value=(0, 4000),
                                  step=100,
                                  format='%d mm',
                                  help='å¥¥è¡Œã®ç¯„å›²ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚')
            with col3:
                # é«˜ã•
                height = st.slider(label='é«˜ã•',
                                   min_value=0,
                                   max_value=3000,
                                   value=(0, 3000),
                                   step=100,
                                   format='%d mm',
                                   help='é«˜ã•ã®ç¯„å›²ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚')
            with col4:
                # åº§é¢é«˜
                sheet_height = st.slider(label='åº§é¢é«˜',
                                         min_value=0,
                                         max_value=3000,
                                         value=(0, 3000),
                                         step=100,
                                         format='%d mm',
                                         help='åº§é¢é«˜ã®ç¯„å›²ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚')

    # ãƒãƒƒãƒ—ã®é¸æŠçµæœã‚’è¡¨ç¤º
    conditions = {
        'ãƒ†ã‚¤ã‚¹ãƒˆ': taste if taste != '' else 'æœªé¸æŠ',
        'ç´æœŸ': delivery if delivery != '' else 'æœªé¸æŠ',
        'ä¾¡æ ¼å¸¯': price if price != '' else 'æœªé¸æŠ',
        'ã‚«ãƒ†ã‚´ãƒª': category if category != '' else 'æœªé¸æŠ',
        'å¹…': witdh,
        'å¥¥è¡Œ': depth,
        'é«˜ã•': height,
        'åº§é¢é«˜': sheet_height
    }
    return conditions


def chat_input():
    prompt = st.chat_input(placeholder="æ¤œç´¢ã—ãŸã„å®¶å…·ã®ã‚¤ãƒ¡ãƒ¼ã‚¸ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚", accept_file=True)
    if prompt:
        # st.write(f"User has sent the following prompt: {prompt}")
        with st.chat_message(name="assistant"):
            st.markdown('''
ã“ã¡ã‚‰ã®å•†å“ã¯ã„ã‹ãŒã§ã—ã‚‡ã†ã‹ï¼Ÿ  
å•†å“åï¼š[PENTE 1P SOFA](https://www.asplund-contract.com/product/12426/)  
ãƒ–ãƒ©ãƒ³ãƒ‰ï¼šWork Plus  
Size:W760 D760 H670 SH425  
Material:Fabric, Steel  
Price:ï¿¥120,000  
![PENTE 1P SOFA](https://www.asplund-contract.com/wp-content/uploads/2024/06/wp_pente1psofa-3-600x600.jpg)
  
  
å•†å“åï¼š[MELTONE 1P SOFA](https://www.asplund-contract.com/product/12396/)  
ãƒ–ãƒ©ãƒ³ãƒ‰ï¼šWork Plus  
Size:W870 D720 H750 SH410
Material:Fabric, Steel  
Price:ï¿¥110,000  
![PENTE 1P SOFA](https://www.asplund-contract.com/wp-content/uploads/2024/06/wp_meltone1psofa-1-600x600.jpg)
''')


def scrapintg():
    url_analyze_flag = False
    scraping_flag = False

    # ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹
    with st.expander(label='ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹', expanded=True):
        data_source_type = sac.segmented(
            items=[
                sac.SegmentedItem(label='XMLã‚µã‚¤ãƒˆãƒãƒƒãƒ—'),
                sac.SegmentedItem(label='URL'),
            ],
            return_index=True,
            label='',
            size='sm',
            radius='sm',
            align='center',
            key='scraping_data_source_type',
        )

        url = ''
        if data_source_type == 0:  # XMLã‚µã‚¤ãƒˆãƒãƒƒãƒ—
            url = st.text_input(label='XMLã‚µã‚¤ãƒˆãƒãƒƒãƒ—ã®URLã‚’å…¥åŠ›ã—ã¦ãã ã•ã„',
                                value='https://www.asplund-contract.com/product-sitemap.xml',
                                placeholder='https://example.com/sitemap.xml',
                                help='XMLã‚µã‚¤ãƒˆãƒãƒƒãƒ—ã®URLã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚')
        elif data_source_type == 1:  # URL
            url = st.text_input(label='URLã‚’å…¥åŠ›ã—ã¦ãã ã•ã„',
                                placeholder='https://example.com',
                                help='ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã—ãŸã„ãƒšãƒ¼ã‚¸ã®URLã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚')
        else:
            url = ''

        # URLè§£æãƒœã‚¿ãƒ³
        url_analyze_flag = st.button(label='URLè§£æ',
                                     icon='ğŸ”',
                                     use_container_width=True,
                                     disabled=True if url == '' else False)

    df_urls = pd.DataFrame()

    # URLè§£æ
    if url_analyze_flag:
        if data_source_type == 0:  # XMLã‚µã‚¤ãƒˆãƒãƒƒãƒ—
            sm = SiteMapParser(url)  # reads /sitemap.xml
            json_exporter = JSONExporter(sm)
            if sm.has_urls():
                df_urls = pd.read_json(io.StringIO(json_exporter.export_urls()))
                df_urls.drop(columns=['lastmod', 'changefreq', 'priority'], inplace=True)
                df_urls.rename(columns={'loc': 'URL'}, inplace=True)
        elif data_source_type == 1:  # URL
            st.write("æœªå®Ÿè£…")
    else:
        df_urls = st.session_state['scraping_all_url_list']

    if len(df_urls) > 0:
        with st.expander(label='URLä¸€è¦§', expanded=True):
            st.session_state['scraping_all_url_list'] = df_urls
            selection = st.dataframe(df_urls,
                                     column_config={
                                         "URL": st.column_config.LinkColumn("URL")
                                     },
                                     use_container_width=True,
                                     hide_index=True,
                                     on_select="rerun",
                                     selection_mode="multi-row").selection
            # é¸æŠã•ã‚ŒãŸURLã‚’ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆã«ä¿å­˜
            if selection.rows:
                st.session_state['scraping_selected_url_list'] = df_urls.iloc[selection.rows, :]
            else:
                st.session_state['scraping_selected_url_list'] = pd.DataFrame()

            # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒœã‚¿ãƒ³
            if len(st.session_state['scraping_selected_url_list']) > 0:
                scraping_flag = st.button(label='ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°', icon='ğŸ•·ï¸', use_container_width=True)

    # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
    if scraping_flag:
        with st.expander(label='ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°', expanded=True):
            asyncio.run(scrape_data(st.session_state['scraping_selected_url_list']['URL'].tolist()))
            st.button(label='ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜', icon='ğŸ’¾', use_container_width=True)


async def scrape_data(urls):
    for url in urls:
        async with AsyncWebCrawler() as crawler:
            result = await crawler.arun(url=url)
            st.write(result.markdown)


if __name__ == "__main__":
    # åˆæœŸåŒ–
    init()
    # ã‚µã‚¤ãƒ‰ãƒãƒ¼ã®è¡¨ç¤º
    menu = sidebar()
    # ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®è¡¨ç¤º
    if menu == 'å®¶å…·é¸å®š':
        # é¸å®šæ¡ä»¶ã®è¡¨ç¤º
        conditions = search_conditions()
        # st.write(conditions)
        # ãƒãƒ£ãƒƒãƒˆå…¥åŠ›
        chat_input()
    elif menu == 'ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°':
        scrapintg()
    elif menu == 'PDFãƒ‡ãƒ¼ã‚¿æŠ½å‡º':
        st.write("æœªå®Ÿè£…")
    else:
        st.write("")

    # with st.expander(label='session', expanded=False):
    #     st.write(st.session_state)
