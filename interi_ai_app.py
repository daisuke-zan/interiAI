import asyncio
import io

import pandas as pd
import streamlit as st
import streamlit_antd_components as sac
from crawl4ai import (AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, LLMConfig)
from crawl4ai.content_scraping_strategy import LXMLWebScrapingStrategy
from crawl4ai.deep_crawling import BFSDeepCrawlStrategy
from crawl4ai.extraction_strategy import LLMExtractionStrategy
from pydantic import BaseModel, Field
from sitemapparser import SiteMapParser
from sitemapparser.exporters import JSONExporter

FURNITURE_CONDITIONS = {
    'ãƒ†ã‚¤ã‚¹ãƒˆ': ['ãƒŠãƒãƒ¥ãƒ©ãƒ«', 'ã‚¤ãƒ³ãƒ€ã‚¹ãƒˆãƒªã‚¢ãƒ«'],
    'ç´æœŸ': ['åœ¨åº«å“', '3é€±é–“', '1ãƒ¶æœˆ', '1.5ã€œ2ãƒ¶æœˆ'],
    'ä¾¡æ ¼å¸¯': ['Low', 'Middle', 'High'],
    'ã‚«ãƒ†ã‚´ãƒª': ['ã‚½ãƒ•ã‚¡', 'ãƒã‚§ã‚¢ãƒ»æ¤…å­'],
    'å¹…': (0, 8000),
    'å¥¥è¡Œ': (0, 4000),
    'é«˜ã•': (0, 3000),
    'åº§é¢é«˜': (0, 3000)
}


def init():
    st.set_page_config(page_title="Interior AI Demo ",
                       page_icon="ğŸª‘",
                       layout="wide",
                       initial_sidebar_state="collapsed")
    # ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆï¼ˆå®¶å…·é¸å®šï¼‰
    if 'furniture_condition_list' not in st.session_state:
        # ã‚«ãƒ©ãƒ åã¨ãƒ‡ãƒ¼ã‚¿å‹ã®è¾æ›¸ã‚’å®šç¾©
        column_types = {}
        for key in FURNITURE_CONDITIONS.keys():
            column_types[key] = str
        # ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆã®DataFrameã‚’åˆæœŸåŒ–
        st.session_state['furniture_condition_list'] = pd.DataFrame({
            col: pd.Series(dtype=dtype) for col, dtype in column_types.items()
        })
    # ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆï¼ˆã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ï¼‰
    if 'scraping_data_source_type' not in st.session_state:
        st.session_state['scraping_data_source_type'] = 0
    if 'scraping_all_url_list' not in st.session_state:
        st.session_state['scraping_all_url_list'] = pd.DataFrame()
    if 'scraping_selected_url_list' not in st.session_state:
        st.session_state['scraping_selected_url_list'] = pd.DataFrame()


def sidebar():
    with st.sidebar:
        selected_menu = sac.menu([
            sac.MenuItem('å®¶å…·é¸å®š', icon='house-door-fill'),
            sac.MenuItem('å•†å“ãƒ‡ãƒ¼ã‚¿ä½œæˆ',
                         icon='database-fill',
                         children=[
                             sac.MenuItem('ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°', icon='globe'),
                             sac.MenuItem('PDFãƒ‡ãƒ¼ã‚¿æŠ½å‡º', icon='filetype-pdf')
                         ])
        ],
                                 open_all=True)
    return selected_menu


def validate_furniture_conditions(conditions):
    # å¿…è¦ãªæ¡ä»¶ãŒã™ã¹ã¦è¨­å®šã•ã‚Œã¦ã„ã‚‹ã‹ã‚’ç¢ºèª
    for key in FURNITURE_CONDITIONS.keys():
        if key in conditions:
            if isinstance(FURNITURE_CONDITIONS[key], tuple):
                # æ•°å€¤ç¯„å›²ã®ãƒã‚§ãƒƒã‚¯
                if not (isinstance(conditions[key], tuple) and len(conditions[key]) == 2 and
                        conditions[key][0] <= conditions[key][1]):
                    print(
                        f"validate_furniture_conditions() paramater invalid : condition {key} is not a valid range."
                    )
                    return False
            elif isinstance(FURNITURE_CONDITIONS[key], list):
                # ãƒªã‚¹ãƒˆã®ãƒã‚§ãƒƒã‚¯
                if not (isinstance(conditions[key], list) and
                        all(isinstance(item, str) for item in conditions[key]) and
                        len(conditions[key]) > 0):
                    print(
                        f"validate_furniture_conditions() paramater invalid : condition {key} is not a valid list."
                    )
                    return False
            else:
                # æ–‡å­—åˆ—ã®ãƒã‚§ãƒƒã‚¯
                if not isinstance(conditions[key], str):
                    return False
        else:
            print(
                f"validate_furniture_conditions() paramater invalid : condition {key} is missing.")
            return False
    return True


def add_furniture_condition_list(conditions):
    if validate_furniture_conditions(conditions) == False:
        st.invalid("é¸å®šæ¡ä»¶ãŒæ­£ã—ãè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        return

    new_condition = {}
    for key in FURNITURE_CONDITIONS.keys():
        if isinstance(conditions[key], tuple) and len(
                conditions[key]) == 2 and conditions[key][0] <= conditions[key][1]:
            # æ•°å€¤ç¯„å›²ã®å ´åˆã€æ–‡å­—åˆ—ã«å¤‰æ›
            new_condition[key] = f"{conditions[key][0]} - {conditions[key][1]} mm"
        else:
            # ãƒªã‚¹ãƒˆã®å ´åˆã€ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã®æ–‡å­—åˆ—ã«å¤‰æ›
            # if isinstance(conditions[key], list):
            #     new_condition[key] = ', '.join(conditions[key])
            # else:   # å˜ä¸€é¸æŠã®å ´åˆ
            #     new_condition[key] = conditions[key]
            new_condition[key] = conditions[key]

    # æ–°ã—ã„è¡Œã‚’DataFrameã¨ã—ã¦ä½œæˆ
    new_row_df = pd.DataFrame([new_condition])

    # æ—¢å­˜ã®DataFrameã«æ–°ã—ã„è¡Œã‚’è¿½åŠ 
    st.session_state['furniture_condition_list'] = pd.concat(
        [st.session_state['furniture_condition_list'], new_row_df], ignore_index=True)


def search_conditions():
    with st.expander(label='é¸å®šæ¡ä»¶', expanded=True):
        col1, col2 = st.columns(spec=2, gap='large', border=False)
        with col1:
            sac.divider(label='æ¡ä»¶è¨­å®š', icon='gear-fill', align='start', color='red')
            # ãƒ†ã‚¤ã‚¹ãƒˆ
            with st.container(border=True):
                taste = sac.chip(
                    items=[sac.ChipItem(label=t) for t in FURNITURE_CONDITIONS['ãƒ†ã‚¤ã‚¹ãƒˆ']],
                    label='ãƒ†ã‚¤ã‚¹ãƒˆ',
                    align='start',
                    radius='xl',
                    variant='outline',
                    multiple=True)

            # ç´æœŸ
            with st.container(border=True):
                delivery = sac.chip(
                    items=[sac.ChipItem(label=t) for t in FURNITURE_CONDITIONS['ç´æœŸ']],
                    label='ç´æœŸ',
                    align='start',
                    radius='xl',
                    variant='outline',
                    multiple=True)

            # ä¾¡æ ¼å¸¯
            with st.container(border=True):
                price = sac.chip(items=[sac.ChipItem(label=t) for t in FURNITURE_CONDITIONS['ä¾¡æ ¼å¸¯']],
                                 label='ä¾¡æ ¼å¸¯',
                                 align='start',
                                 radius='xl',
                                 variant='outline',
                                 multiple=True)
            # ã‚«ãƒ†ã‚´ãƒª
            with st.container(border=True):
                category = sac.cascader(items=[
                    sac.CasItem('ã‚½ãƒ•ã‚¡',
                                children=[
                                    sac.CasItem('1äººæ›ã‘ã‚½ãƒ•ã‚¡'),
                                    sac.CasItem('2äººæ›ã‘ã‚½ãƒ•ã‚¡'),
                                    sac.CasItem('3äººæ›ã‘ã‚½ãƒ•ã‚¡'),
                                ]),
                    sac.CasItem('ãƒã‚§ã‚¢ãƒ»æ¤…å­',
                                children=[
                                    sac.CasItem('ã‚ªãƒ•ã‚£ã‚¹ãƒã‚§ã‚¢ãƒ»ãƒ¯ãƒ¼ã‚¯ãƒã‚§ã‚¢'),
                                    sac.CasItem('ãƒŸãƒ¼ãƒ†ã‚£ãƒ³ã‚°ãƒã‚§ã‚¢'),
                                    sac.CasItem('ãƒ€ã‚¤ãƒ‹ãƒ³ã‚°ãƒã‚§ã‚¢'),
                                ]),
                ],
                                        label='ã‚«ãƒ†ã‚´ãƒª',
                                        placeholder='ã‚«ãƒ†ã‚´ãƒªã‚’é¸æŠã—ã¦ãã ã•ã„ï¼ˆã¾ã å…¨ã‚«ãƒ†ã‚´ãƒªå…¥ã‚Œã¦ã¾ã›ã‚“ï¼‰',
                                        multiple=True,
                                        search=True,
                                        clear=True)
            # ã‚µã‚¤ã‚º
            with st.container(border=True):
                st.caption('ã‚µã‚¤ã‚º')
                col1_1, col1_2 = st.columns(spec=2, gap='large', border=False)
                with col1_1:
                    # å¹…
                    witdh = st.slider(label='å¹…',
                                      min_value=FURNITURE_CONDITIONS['å¹…'][0],
                                      max_value=FURNITURE_CONDITIONS['å¹…'][1],
                                      value=(FURNITURE_CONDITIONS['å¹…'][0],
                                             FURNITURE_CONDITIONS['å¹…'][1]),
                                      step=100,
                                      format='%d mm',
                                      help='å¹…ã®ç¯„å›²ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚')
                    # å¥¥è¡Œ
                    depth = st.slider(label='å¥¥è¡Œ',
                                      min_value=FURNITURE_CONDITIONS['å¥¥è¡Œ'][0],
                                      max_value=FURNITURE_CONDITIONS['å¥¥è¡Œ'][1],
                                      value=(FURNITURE_CONDITIONS['å¥¥è¡Œ'][0],
                                             FURNITURE_CONDITIONS['å¥¥è¡Œ'][1]),
                                      step=100,
                                      format='%d mm',
                                      help='å¥¥è¡Œã®ç¯„å›²ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚')
                with col1_2:
                    # é«˜ã•
                    height = st.slider(label='é«˜ã•',
                                       min_value=FURNITURE_CONDITIONS['é«˜ã•'][0],
                                       max_value=FURNITURE_CONDITIONS['é«˜ã•'][1],
                                       value=(FURNITURE_CONDITIONS['é«˜ã•'][0],
                                              FURNITURE_CONDITIONS['é«˜ã•'][1]),
                                       step=100,
                                       format='%d mm',
                                       help='é«˜ã•ã®ç¯„å›²ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚')
                    # åº§é¢é«˜
                    sheet_height = st.slider(label='åº§é¢é«˜',
                                             min_value=FURNITURE_CONDITIONS['åº§é¢é«˜'][0],
                                             max_value=FURNITURE_CONDITIONS['åº§é¢é«˜'][1],
                                             value=(FURNITURE_CONDITIONS['åº§é¢é«˜'][0],
                                                    FURNITURE_CONDITIONS['åº§é¢é«˜'][1]),
                                             step=100,
                                             format='%d mm',
                                             help='åº§é¢é«˜ã®ç¯„å›²ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚')
            # æ¡ä»¶ã‚’ã¾ã¨ã‚ã‚‹
            conditions = {
                'ãƒ†ã‚¤ã‚¹ãƒˆ': taste,
                'ç´æœŸ': delivery,
                'ä¾¡æ ¼å¸¯': price,
                'ã‚«ãƒ†ã‚´ãƒª': category,
                'å¹…': witdh,
                'å¥¥è¡Œ': depth,
                'é«˜ã•': height,
                'åº§é¢é«˜': sheet_height
            }

            if st.button(label='é¸å®šæ¡ä»¶ã‚’ä¿å­˜',
                         icon='ğŸ’¾',
                         disabled=not validate_furniture_conditions(conditions),
                         use_container_width=True):
                # é¸å®šæ¡ä»¶ã‚’ä¿å­˜
                add_furniture_condition_list(conditions)
        with col2:
            sac.divider(label='æ¡ä»¶ä¸€è¦§', icon='card-checklist', align='start', color='red')
            # é¸å®šæ¡ä»¶ä¸€è¦§
            with st.container(border=True):
                st.dataframe(data=st.session_state['furniture_condition_list'],
                             key='furniture_condition_list_df',
                             hide_index=True,
                             on_select="rerun",
                             selection_mode="multi-row")
                # é¸æŠã•ã‚ŒãŸè¡Œã‚’å‰Šé™¤
                selected_rows_indices = st.session_state.furniture_condition_list_df["selection"][
                    "rows"]
                if st.button(label="é¸æŠã•ã‚ŒãŸã‚¢ã‚¤ãƒ†ãƒ ã‚’å‰Šé™¤",
                             icon='ğŸ—‘ï¸',
                             disabled=not selected_rows_indices,
                             use_container_width=True):
                    if selected_rows_indices:
                        # é¸æŠã•ã‚ŒãŸè¡Œã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«åŸºã¥ã„ã¦DataFrameã‹ã‚‰è¡Œã‚’å‰Šé™¤
                        # drop() ãƒ¡ã‚½ãƒƒãƒ‰ã§æŒ‡å®šã—ãŸã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®è¡Œã‚’å‰Šé™¤ã—ã¾ã™
                        st.session_state['furniture_condition_list'] = \
                            st.session_state['furniture_condition_list'].drop(selected_rows_indices).reset_index(drop=True)
                        st.rerun()  # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’æ›´æ–°ã™ã‚‹ãŸã‚ã«å†å®Ÿè¡Œ
                    else:
                        st.warning("å‰Šé™¤ã™ã‚‹ã‚¢ã‚¤ãƒ†ãƒ ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚")


def chat_input():
    prompt = st.chat_input(placeholder="ãã®ä»–ã«ã‚‚ã”è¦æœ›ãŒã‚ã‚Œã°å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚", accept_file=True)
    if prompt:
        # st.write(f"User has sent the following prompt: {prompt}")
        with st.chat_message(name="assistant"):
            st.markdown('''
ã“ã¡ã‚‰ã®å•†å“ã¯ã„ã‹ãŒã§ã—ã‚‡ã†ã‹ï¼Ÿ  
å•†å“åï¼š[PENTE 1P SOFA](https://www.asplund-contract.com/product/12426/)  
ãƒ–ãƒ©ãƒ³ãƒ‰ï¼šWork Plus  
Size:W760 D760 H670 SH425  
Material:Fabric, Steel  
Price:ï¿¥120,000  
![PENTE 1P SOFA](https://www.asplund-contract.com/wp-content/uploads/2024/06/wp_pente1psofa-3-600x600.jpg)
  
  
å•†å“åï¼š[MELTONE 1P SOFA](https://www.asplund-contract.com/product/12396/)  
ãƒ–ãƒ©ãƒ³ãƒ‰ï¼šWork Plus  
Size:W870 D720 H750 SH410
Material:Fabric, Steel  
Price:ï¿¥110,000  
![PENTE 1P SOFA](https://www.asplund-contract.com/wp-content/uploads/2024/06/wp_meltone1psofa-1-600x600.jpg)
''')


def scrapintg():
    url_analyze_flag = False
    scraping_flag = False

    # ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹
    with st.expander(label='ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹', expanded=True):
        data_source_type = sac.segmented(
            items=[
                sac.SegmentedItem(label='XMLã‚µã‚¤ãƒˆãƒãƒƒãƒ—'),
                sac.SegmentedItem(label='URL'),
            ],
            return_index=True,
            label='',
            size='sm',
            radius='sm',
            align='center',
            key='scraping_data_source_type',
        )

        url = ''
        if data_source_type == 0:  # XMLã‚µã‚¤ãƒˆãƒãƒƒãƒ—
            url = st.text_input(label='XMLã‚µã‚¤ãƒˆãƒãƒƒãƒ—ã®URLã‚’å…¥åŠ›ã—ã¦ãã ã•ã„',
                                value='https://www.asplund-contract.com/product-sitemap.xml',
                                placeholder='https://example.com/sitemap.xml',
                                help='XMLã‚µã‚¤ãƒˆãƒãƒƒãƒ—ã®URLã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚')
        elif data_source_type == 1:  # URL
            url = st.text_input(label='URLã‚’å…¥åŠ›ã—ã¦ãã ã•ã„',
                                value='https://www.asplund-contract.com/product/br/workplus/',
                                placeholder='https://example.com',
                                help='ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã—ãŸã„ãƒšãƒ¼ã‚¸ã®URLã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚')
        else:
            url = ''

        # URLè§£æãƒœã‚¿ãƒ³
        url_analyze_flag = st.button(label='URLè§£æ',
                                     icon='ğŸ”',
                                     use_container_width=True,
                                     disabled=True if url == '' else False)

    # URLä¸€è¦§ã®DataFrame
    df_urls = pd.DataFrame()

    # URLè§£æ
    if url_analyze_flag:
        if data_source_type == 0:  # XMLã‚µã‚¤ãƒˆãƒãƒƒãƒ—
            sm = SiteMapParser(url)  # reads /sitemap.xml
            json_exporter = JSONExporter(sm)
            if sm.has_urls():
                df_urls = pd.read_json(io.StringIO(json_exporter.export_urls()))
                df_urls.drop(columns=['lastmod', 'changefreq', 'priority'], inplace=True)
                df_urls.rename(columns={'loc': 'URL'}, inplace=True)
        elif data_source_type == 1:  # URL
            # asyncio.run(scrape_item_list(url))
            st.write("æœªå®Ÿè£…")

    else:
        df_urls = st.session_state['scraping_all_url_list']

    if len(df_urls) > 0:
        with st.expander(label='URLä¸€è¦§', expanded=True):
            st.session_state['scraping_all_url_list'] = df_urls
            selection = st.dataframe(df_urls,
                                     column_config={
                                         "URL": st.column_config.LinkColumn("URL")
                                     },
                                     use_container_width=True,
                                     hide_index=True,
                                     on_select="rerun",
                                     selection_mode="multi-row").selection
            # é¸æŠã•ã‚ŒãŸURLã‚’ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆã«ä¿å­˜
            if selection.rows:
                st.session_state['scraping_selected_url_list'] = df_urls.iloc[selection.rows, :]
            else:
                st.session_state['scraping_selected_url_list'] = pd.DataFrame()

            # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒœã‚¿ãƒ³
            if len(st.session_state['scraping_selected_url_list']) > 0:
                scraping_flag = st.button(label='ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°', icon='ğŸ•·ï¸', use_container_width=True)

    # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
    if scraping_flag:
        with st.expander(label='ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°', expanded=True):
            scraped_data = asyncio.run(
                scrape_data(st.session_state['scraping_selected_url_list']['URL'].tolist()))
            st.text_area(label='ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°çµæœ', value=scraped_data, height=300, disabled=True)
            st.button(label='ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜', icon='ğŸ’¾', use_container_width=True)


async def scrape_data(urls):

    class ProductInfo(BaseModel):
        brand_name: str = Field(..., description="ãƒ–ãƒ©ãƒ³ãƒ‰åã‚’è¡¨ã™æ–‡å­—åˆ—")
        item_name: str = Field(..., description="ã‚¢ã‚¤ãƒ†ãƒ åã‚’è¡¨ã™æ–‡å­—åˆ—")
        size: str = Field(..., description="ã‚µã‚¤ã‚ºã‚’è¡¨ã™æ–‡å­—åˆ—. ä¾‹ï¼šW500 D500 H550 ")
        weight: str = Field(..., description="é‡é‡ã‚’è¡¨ã™æ–‡å­—åˆ—. ä¾‹ï¼š15kg")
        material: str = Field(..., description="ç´ æã‚’è¡¨ã™æ–‡å­—åˆ—. ä¾‹ï¼šFabric, Steel")
        price: str = Field(..., description="ä¾¡æ ¼ã‚’è¡¨ã™æ–‡å­—åˆ—. ä¾‹ï¼šï¿¥120,000ï¼ˆç¨è¾¼ï¼‰")
        description: str = Field(...,
                                 description="å•†å“èª¬æ˜ã‚’è¡¨ã™æ–‡å­—åˆ—. ä¾‹ï¼šã“ã®ã‚½ãƒ•ã‚¡ã¯ã€å¿«é©ãªåº§ã‚Šå¿ƒåœ°ã¨ã‚¹ã‚¿ã‚¤ãƒªãƒƒã‚·ãƒ¥ãªãƒ‡ã‚¶ã‚¤ãƒ³ã‚’å…¼ã­å‚™ãˆã¦ã„ã¾ã™ã€‚")
        image_urls: list[str] = Field(..., description="å•†å“ç”»åƒã®URLãƒªã‚¹ãƒˆ")  # ç”»åƒã®URLãƒªã‚¹ãƒˆ

    browser_config = BrowserConfig(
        user_agent_mode="random",  # ãƒœãƒƒãƒˆæ¤œå‡ºã«å¯¾æŠ—
        verbose=True,
        # headless=False
    )
    # Define the LLM extraction strategy
    llm_strategy = LLMExtractionStrategy(
        llm_config=LLMConfig(provider="gemini/gemini-2.5-flash",
                             api_token=st.secrets['GEMINI_API_TOKEN']),
        schema=ProductInfo.model_json_schema(),
        extraction_type="schema",
        # extraction_type="block",
        verbose=True,
        instruction='''
ä¸ãˆã‚‰ã‚ŒãŸã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ã€ä»¥ä¸‹ã®JSONå½¢å¼ã«å¤‰æ›ã—ã¦å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚\n\n
ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚³ãƒ¼ãƒ‰ã§ã¯ãªãã€JSONå½¢å¼ã®ãƒ‡ãƒ¼ã‚¿ã‚’å‡ºåŠ›ã—ã¾ã™ã€‚
JSONä»¥å¤–ã®æ–‡å­—ã‚„ã‚³ãƒ¡ãƒ³ãƒˆã¯çµ¶å¯¾ã«çµ¶å¯¾ã«çµ¶å¯¾ã«å‡ºåŠ›ã—ãªã„ã§ãã ã•ã„ã€‚

# ä»¥ä¸‹ã®æ¡ä»¶ã‚’å¿…ãšå®ˆã£ã¦ãã ã•ã„:
- çµæœã¯å³å¯†ãªJSONã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’è¿”ã™ã“ã¨
- JSONä»¥å¤–ã®æ–‡å­—ã‚„ã‚³ãƒ¡ãƒ³ãƒˆã¯çµ¶å¯¾ã«çµ¶å¯¾ã«çµ¶å¯¾ã«å‡ºåŠ›ã—ãªã„
# å‡ºåŠ›ä¾‹
{
  "brand_name": "ã“ã“ã«ãƒ–ãƒ©ãƒ³ãƒ‰å",
  "item_name": "ã“ã“ã«å•†å“åã‚’å‡ºåŠ›",
  "size": "ã“ã“ã«ã‚µã‚¤ã‚ºã‚’mmå˜ä½ã«å¤‰æ›ã—ã¦å‡ºåŠ›",
  "weight": "ã“ã“ã«é‡é‡ã‚’kgå˜ä½ã«å¤‰æ›ã—ã¦å‡ºåŠ›",
  "material": "ã“ã“ã«ç´ æã‚’å‡ºåŠ›",
  "price": "ã“ã“ã«ä¾¡æ ¼ã‚’æ—¥æœ¬å††ã§å‡ºåŠ›ï¼ˆç¨è¾¼/ç¨æŠœãŒã‚ã‹ã‚‹ã‚ˆã†ã«è¨˜è¼‰ï¼‰",
  "description": "ã“ã“ã«å•†å“èª¬æ˜ã‚’å‡ºåŠ›",
  "image_urls": [
    "https://example.com/images/item1.jpg",
    "https://example.com/images/item2.jpg",
    "https://example.com/images/item3.jpg"
  ]
}
''',
        chunk_token_threshold=65536,
        overlap_rate=0.0,
        apply_chunking=True,
        input_format="markdown",  # or "html", "fit_markdown"
        extra_args={
            "temperature": 0.0,
            "max_tokens": 65536
        })
    run_config = CrawlerRunConfig(
        excluded_tags=['header', 'footer', 'nav'],  # é™¤å¤–ã™ã‚‹ã‚¿ã‚°
        extraction_strategy=llm_strategy,
        # word_count_threshold=10,  # Minimum words per content block. ã‚µã‚¤ãƒˆã«çŸ­ã„æ®µè½ã‚„é …ç›®ãŒå¤šæ•°ã‚ã‚‹å ´åˆã€ãƒ–ãƒ­ãƒƒã‚¯ãŒè€ƒæ…®ã•ã‚Œã‚‹å‰ã®æœ€å°å˜èªæ•°ã‚’ä¸‹ã’ã‚‹
        exclude_external_links=True,  # Remove external links
        exclude_social_media_links=True,  # Remove social media links
        exclude_external_images=True,  # Remove external images
        remove_overlay_elements=True,  # Remove popups/modals
        process_iframes=True,  # Process iframe content
        verbose=True)

    scraped_data = ''
    for url in urls:
        async with AsyncWebCrawler(config=browser_config) as crawler:
            result = await crawler.arun(url=url, config=run_config)
            if result.success:
                # st.write("markdown")
                # st.write(result.markdown)
                # LLMExtractionStrategyã‚’ä½¿ç”¨ã—ã¦ã„ã‚‹å ´åˆã€result.markdownã«æŠ½å‡ºã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ãŒå«ã¾ã‚Œã‚‹
                st.write(f"LLM: \n\n{result.extracted_content}")
                st.write(f"markdown: \n\n{result.markdown}")
                scraped_data += f"{result.markdown}\n\n"
                # st.write("media")
                # for image in result.media["images"]:
                #     st.image(image['src'], caption=image['alt'])
                # st.write("links")
                # for link in result.links["internal"]:
                #     st.markdown(f"[{link['text']}]({link['href']})")
            else:
                st.invalid(
                    f"Failed to scrape {url}: status_code:{result.status_code} message:{result.invalid_message}"
                )
                continue

    return scraped_data


async def scrape_item_list(url):
    browser_config = BrowserConfig(
        user_agent_mode="random",  # ãƒœãƒƒãƒˆæ¤œå‡ºã«å¯¾æŠ—
        verbose=True,
        # headless=False
    )
    run_config = CrawlerRunConfig(
        excluded_tags=['header', 'footer', 'nav'],  # é™¤å¤–ã™ã‚‹ã‚¿ã‚°
        # word_count_threshold=10,  # Minimum words per content block. ã‚µã‚¤ãƒˆã«çŸ­ã„æ®µè½ã‚„é …ç›®ãŒå¤šæ•°ã‚ã‚‹å ´åˆã€ãƒ–ãƒ­ãƒƒã‚¯ãŒè€ƒæ…®ã•ã‚Œã‚‹å‰ã®æœ€å°å˜èªæ•°ã‚’ä¸‹ã’ã‚‹
        exclude_external_links=True,  # Remove external links
        exclude_social_media_links=True,  # Remove social media links
        exclude_external_images=True,  # Remove external images
        remove_overlay_elements=True,  # Remove popups/modals
        process_iframes=True  # Process iframe content
    )

    async with AsyncWebCrawler(config=browser_config) as crawler:
        result = await crawler.arun(url=url, config=run_config)
        if result.success:
            st.write("markdown")
            st.write(result.markdown)
            # st.write("media")
            # for image in result.media["images"]:
            #     st.image(image['src'], caption=image['alt'])
            # st.write("links")
            # for link in result.links["internal"]:
            #     st.markdown(f"[{link['text']}]({link['href']})")
        else:
            st.invalid(
                f"Failed to scrape {url}: status_code:{result.status_code} message:{result.invalid_message}"
            )


async def deep_crawl_test(url):
    config = CrawlerRunConfig(deep_crawl_strategy=BFSDeepCrawlStrategy(max_depth=0,
                                                                       include_external=False),
                              scraping_strategy=LXMLWebScrapingStrategy(),
                              verbose=True)

    async with AsyncWebCrawler() as crawler:
        results = await crawler.arun(url, config=config)

        st.write(f"Crawled {len(results)} pages in total")

        # Access individual results
        for result in results[:3]:  # Show first 3 results
            st.write(f"URL: {result.url}")
            st.write(f"Depth: {result.metadata.get('depth', 0)}")
            st.write(result.markdown)  # Show first 500 characters of markdown


if __name__ == "__main__":
    # åˆæœŸåŒ–
    init()
    # ã‚µã‚¤ãƒ‰ãƒãƒ¼ã®è¡¨ç¤º
    menu = sidebar()
    # ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®è¡¨ç¤º
    if menu == 'å®¶å…·é¸å®š':
        # é¸å®šæ¡ä»¶ã®è¡¨ç¤º
        search_conditions()
        #
        if st.button(label='ä¿å­˜ã—ãŸæ¡ä»¶ã§æ¤œç´¢',
                     icon='ğŸ”',
                     disabled=len(st.session_state['furniture_condition_list']) == 0,
                     use_container_width=True):
            with st.chat_message(name="assistant"):
                st.markdown('''
    ã“ã¡ã‚‰ã®å•†å“ã¯ã„ã‹ãŒã§ã—ã‚‡ã†ã‹ï¼Ÿ  
    å•†å“åï¼š[PENTE 1P SOFA](https://www.asplund-contract.com/product/12426/)  
    ãƒ–ãƒ©ãƒ³ãƒ‰ï¼šWork Plus  
    Size:W760 D760 H670 SH425  
    Material:Fabric, Steel  
    Price:ï¿¥120,000  
    ![PENTE 1P SOFA](https://www.asplund-contract.com/wp-content/uploads/2024/06/wp_pente1psofa-3-600x600.jpg)
    
    
    å•†å“åï¼š[MELTONE 1P SOFA](https://www.asplund-contract.com/product/12396/)  
    ãƒ–ãƒ©ãƒ³ãƒ‰ï¼šWork Plus  
    Size:W870 D720 H750 SH410
    Material:Fabric, Steel  
    Price:ï¿¥110,000  
    ![PENTE 1P SOFA](https://www.asplund-contract.com/wp-content/uploads/2024/06/wp_meltone1psofa-1-600x600.jpg)
    ''')

        # ãƒãƒ£ãƒƒãƒˆå…¥åŠ›
        chat_input()
    elif menu == 'ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°':
        scrapintg()
    elif menu == 'PDFãƒ‡ãƒ¼ã‚¿æŠ½å‡º':
        st.write("æœªå®Ÿè£…")
    else:
        st.write("")

    # with st.expander(label='session', expanded=False):
    #     st.write(st.session_state)
