import asyncio
import io

import pandas as pd
import streamlit as st
import streamlit_antd_components as sac
from crawl4ai import (AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, LLMConfig)
from crawl4ai.content_scraping_strategy import LXMLWebScrapingStrategy
from crawl4ai.deep_crawling import BFSDeepCrawlStrategy
from crawl4ai.extraction_strategy import LLMExtractionStrategy
from pydantic import BaseModel, Field
from sitemapparser import SiteMapParser
from sitemapparser.exporters import JSONExporter


def init():
    st.set_page_config(page_title="Interior AI Demo ",
                       page_icon="ğŸª‘",
                       layout="wide",
                       initial_sidebar_state="collapsed")
    # ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆï¼ˆå®¶å…·é¸å®šï¼‰
    if 'furniture_condition_list' not in st.session_state:
        # ã‚«ãƒ©ãƒ åã¨ãƒ‡ãƒ¼ã‚¿å‹ã®è¾æ›¸ã‚’å®šç¾©
        column_types = {
            'ãƒ†ã‚¤ã‚¹ãƒˆ': str,
            'ç´æœŸ': str,
            'ä¾¡æ ¼å¸¯': str,
            'ã‚«ãƒ†ã‚´ãƒª': str,
            'å¹…': str,
            'å¥¥è¡Œ': str,
            'é«˜ã•': str,
            'åº§é¢é«˜': str
        }
        st.session_state['furniture_condition_list'] = pd.DataFrame({
            col: pd.Series(dtype=dtype) for col, dtype in column_types.items()
        })
    # ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆï¼ˆã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ï¼‰
    if 'scraping_data_source_type' not in st.session_state:
        st.session_state['scraping_data_source_type'] = 0
    if 'scraping_all_url_list' not in st.session_state:
        st.session_state['scraping_all_url_list'] = pd.DataFrame()
    if 'scraping_selected_url_list' not in st.session_state:
        st.session_state['scraping_selected_url_list'] = pd.DataFrame()


def sidebar():
    with st.sidebar:
        selected_menu = sac.menu([
            sac.MenuItem('å®¶å…·é¸å®š', icon='house-door-fill'),
            sac.MenuItem('å•†å“ãƒ‡ãƒ¼ã‚¿ä½œæˆ',
                         icon='database-fill',
                         children=[
                             sac.MenuItem('ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°', icon='globe'),
                             sac.MenuItem('PDFãƒ‡ãƒ¼ã‚¿æŠ½å‡º', icon='filetype-pdf')
                         ])
        ],
                                 open_all=True)
    return selected_menu


def validate_furniture_conditions(conditions):
    # å¿…è¦ãªæ¡ä»¶ãŒã™ã¹ã¦è¨­å®šã•ã‚Œã¦ã„ã‚‹ã‹ã‚’ç¢ºèª
    required_str_conditions = ['ãƒ†ã‚¤ã‚¹ãƒˆ', 'ç´æœŸ']
    required_list_conditions = ['ãƒ†ã‚¤ã‚¹ãƒˆ', 'ç´æœŸ', 'ä¾¡æ ¼å¸¯', 'ã‚«ãƒ†ã‚´ãƒª', 'å¹…', 'å¥¥è¡Œ', 'é«˜ã•', 'åº§é¢é«˜']
    for condition in required_str_conditions:
        if condition not in conditions or conditions[condition] == '':
            return False
    for condition in required_list_conditions:
        if condition not in conditions or (isinstance(conditions[condition], list) and
                                           len(conditions[condition]) == 0):
            return False
    return True


def add_furniture_condition_list(conditions):
    if validate_furniture_conditions(conditions) == False:
        st.error("é¸å®šæ¡ä»¶ãŒæ­£ã—ãè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        return

    new_condition = {
        'ãƒ†ã‚¤ã‚¹ãƒˆ': conditions['ãƒ†ã‚¤ã‚¹ãƒˆ'],
        'ç´æœŸ': conditions['ç´æœŸ'],
        'ä¾¡æ ¼å¸¯': conditions['ä¾¡æ ¼å¸¯'],
        'ã‚«ãƒ†ã‚´ãƒª': conditions['ã‚«ãƒ†ã‚´ãƒª'],
        'å¹…': f"{conditions['å¹…'][0]} - {conditions['å¹…'][1]} mm",
        'å¥¥è¡Œ': f"{conditions['å¥¥è¡Œ'][0]} - {conditions['å¥¥è¡Œ'][1]} mm",
        'é«˜ã•': f"{conditions['é«˜ã•'][0]} - {conditions['é«˜ã•'][1]} mm",
        'åº§é¢é«˜': f"{conditions['åº§é¢é«˜'][0]} - {conditions['åº§é¢é«˜'][1]} mm"
    }
    # æ–°ã—ã„è¡Œã‚’DataFrameã¨ã—ã¦ä½œæˆ
    new_row_df = pd.DataFrame([new_condition])

    # æ—¢å­˜ã®DataFrameã«æ–°ã—ã„è¡Œã‚’è¿½åŠ 
    st.session_state['furniture_condition_list'] = pd.concat(
        [st.session_state['furniture_condition_list'], new_row_df], ignore_index=True)


def search_conditions():
    with st.expander(label='é¸å®šæ¡ä»¶', expanded=True):
        col1, col2 = st.columns(spec=2, gap='large', border=False)
        with col1:
            sac.divider(label='æ¡ä»¶è¨­å®š', icon='gear-fill', align='start', color='red')
            # ãƒ†ã‚¤ã‚¹ãƒˆ
            with st.container(border=True):
                taste = sac.chip(items=[
                    sac.ChipItem(label='ãƒŠãƒãƒ¥ãƒ©ãƒ«', icon='tree-fill'),
                    sac.ChipItem(label='ã‚¤ãƒ³ãƒ€ã‚¹ãƒˆãƒªã‚¢ãƒ«', icon='building-fill'),
                ],
                                 label='ãƒ†ã‚¤ã‚¹ãƒˆ',
                                 key='furniture_taste_chip',
                                 align='start',
                                 radius='xl',
                                 variant='light',
                                 multiple=False)

            # ç´æœŸ
            with st.container(border=True):
                delivery = sac.chip(items=[
                    sac.ChipItem(label='åœ¨åº«å“', icon='truck-fill'),
                    sac.ChipItem(label='3é€±é–“', icon='calendar-check-fill'),
                    sac.ChipItem(label='1ãƒ¶æœˆ', icon='calendar-check-fill'),
                    sac.ChipItem(label='1.5ã€œ2ãƒ¶æœˆ', icon='calendar-check-fill'),
                ],
                                    label='ç´æœŸ',
                                    key='furniture_delivery_chip',
                                    align='start',
                                    radius='xl',
                                    variant='outline',
                                    multiple=False)

            # ä¾¡æ ¼å¸¯
            with st.container(border=True):
                price = sac.chip(items=[
                    sac.ChipItem(label='Low', icon='cash-coin'),
                    sac.ChipItem(label='Middle', icon='cash-coin'),
                    sac.ChipItem(label='High', icon='cash-coin')
                ],
                                 label='ä¾¡æ ¼å¸¯',
                                 key='furniture_price_chip',
                                 align='start',
                                 radius='xl',
                                 variant='outline',
                                 multiple=True)
            # ã‚«ãƒ†ã‚´ãƒª
            with st.container(border=True):
                category = sac.cascader(items=[
                    sac.CasItem('ã‚½ãƒ•ã‚¡',
                                children=[
                                    sac.CasItem('1äººæ›ã‘ã‚½ãƒ•ã‚¡'),
                                    sac.CasItem('2äººæ›ã‘ã‚½ãƒ•ã‚¡'),
                                    sac.CasItem('3äººæ›ã‘ã‚½ãƒ•ã‚¡'),
                                ]),
                    sac.CasItem('ãƒã‚§ã‚¢ãƒ»æ¤…å­',
                                children=[
                                    sac.CasItem('ã‚ªãƒ•ã‚£ã‚¹ãƒã‚§ã‚¢ãƒ»ãƒ¯ãƒ¼ã‚¯ãƒã‚§ã‚¢'),
                                    sac.CasItem('ãƒŸãƒ¼ãƒ†ã‚£ãƒ³ã‚°ãƒã‚§ã‚¢'),
                                    sac.CasItem('ãƒ€ã‚¤ãƒ‹ãƒ³ã‚°ãƒã‚§ã‚¢'),
                                ]),
                ],
                                        label='ã‚«ãƒ†ã‚´ãƒª',
                                        key='furniture_category_cascader',
                                        placeholder='ã‚«ãƒ†ã‚´ãƒªã‚’é¸æŠã—ã¦ãã ã•ã„ï¼ˆã¾ã å…¨ã‚«ãƒ†ã‚´ãƒªå…¥ã‚Œã¦ã¾ã›ã‚“ï¼‰',
                                        multiple=True,
                                        search=True,
                                        clear=True)
            # ã‚µã‚¤ã‚º
            with st.container(border=True):
                st.caption('ã‚µã‚¤ã‚º')
                col1_1, col1_2 = st.columns(spec=2, gap='large', border=False)
                with col1_1:
                    # å¹…
                    witdh = st.slider(label='å¹…',
                                      key='furniture_width_slider',
                                      min_value=0,
                                      max_value=8000,
                                      value=(0, 8000),
                                      step=100,
                                      format='%d mm',
                                      help='å¹…ã®ç¯„å›²ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚')
                    # å¥¥è¡Œ
                    depth = st.slider(label='å¥¥è¡Œ',
                                      key='furniture_depth_slider',
                                      min_value=0,
                                      max_value=4000,
                                      value=(0, 4000),
                                      step=100,
                                      format='%d mm',
                                      help='å¥¥è¡Œã®ç¯„å›²ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚')
                with col1_2:
                    # é«˜ã•
                    height = st.slider(label='é«˜ã•',
                                       key='furniture_height_slider',
                                       min_value=0,
                                       max_value=3000,
                                       value=(0, 3000),
                                       step=100,
                                       format='%d mm',
                                       help='é«˜ã•ã®ç¯„å›²ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚')
                    # åº§é¢é«˜
                    sheet_height = st.slider(label='åº§é¢é«˜',
                                             key='furniture_sheet_height_slider',
                                             min_value=0,
                                             max_value=3000,
                                             value=(0, 3000),
                                             step=100,
                                             format='%d mm',
                                             help='åº§é¢é«˜ã®ç¯„å›²ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚')
            # æ¡ä»¶ã‚’ã¾ã¨ã‚ã‚‹
            conditions = {
                'ãƒ†ã‚¤ã‚¹ãƒˆ': taste,
                'ç´æœŸ': delivery,
                'ä¾¡æ ¼å¸¯': price,
                'ã‚«ãƒ†ã‚´ãƒª': category,
                'å¹…': witdh,
                'å¥¥è¡Œ': depth,
                'é«˜ã•': height,
                'åº§é¢é«˜': sheet_height
            }

            if st.button(label='é¸å®šæ¡ä»¶ã‚’ä¿å­˜',
                         icon='ğŸ’¾',
                         disabled=not validate_furniture_conditions(conditions),
                         use_container_width=True):
                # é¸å®šæ¡ä»¶ã‚’ä¿å­˜
                add_furniture_condition_list(conditions)
        with col2:
            sac.divider(label='æ¡ä»¶ä¸€è¦§', icon='card-checklist', align='start', color='red')
            # é¸å®šæ¡ä»¶ä¸€è¦§
            with st.container(border=True):
                st.dataframe(st.session_state['furniture_condition_list'],
                             hide_index=True,
                             on_select="rerun",
                             selection_mode="multi-row")


def chat_input():
    prompt = st.chat_input(placeholder="ãã®ä»–ã«ã‚‚ã”è¦æœ›ãŒã‚ã‚Œã°å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚", accept_file=True)
    if prompt:
        # st.write(f"User has sent the following prompt: {prompt}")
        with st.chat_message(name="assistant"):
            st.markdown('''
ã“ã¡ã‚‰ã®å•†å“ã¯ã„ã‹ãŒã§ã—ã‚‡ã†ã‹ï¼Ÿ  
å•†å“åï¼š[PENTE 1P SOFA](https://www.asplund-contract.com/product/12426/)  
ãƒ–ãƒ©ãƒ³ãƒ‰ï¼šWork Plus  
Size:W760 D760 H670 SH425  
Material:Fabric, Steel  
Price:ï¿¥120,000  
![PENTE 1P SOFA](https://www.asplund-contract.com/wp-content/uploads/2024/06/wp_pente1psofa-3-600x600.jpg)
  
  
å•†å“åï¼š[MELTONE 1P SOFA](https://www.asplund-contract.com/product/12396/)  
ãƒ–ãƒ©ãƒ³ãƒ‰ï¼šWork Plus  
Size:W870 D720 H750 SH410
Material:Fabric, Steel  
Price:ï¿¥110,000  
![PENTE 1P SOFA](https://www.asplund-contract.com/wp-content/uploads/2024/06/wp_meltone1psofa-1-600x600.jpg)
''')


def scrapintg():
    url_analyze_flag = False
    scraping_flag = False

    # ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹
    with st.expander(label='ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹', expanded=True):
        data_source_type = sac.segmented(
            items=[
                sac.SegmentedItem(label='XMLã‚µã‚¤ãƒˆãƒãƒƒãƒ—'),
                sac.SegmentedItem(label='URL'),
            ],
            return_index=True,
            label='',
            size='sm',
            radius='sm',
            align='center',
            key='scraping_data_source_type',
        )

        url = ''
        if data_source_type == 0:  # XMLã‚µã‚¤ãƒˆãƒãƒƒãƒ—
            url = st.text_input(label='XMLã‚µã‚¤ãƒˆãƒãƒƒãƒ—ã®URLã‚’å…¥åŠ›ã—ã¦ãã ã•ã„',
                                value='https://www.asplund-contract.com/product-sitemap.xml',
                                placeholder='https://example.com/sitemap.xml',
                                help='XMLã‚µã‚¤ãƒˆãƒãƒƒãƒ—ã®URLã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚')
        elif data_source_type == 1:  # URL
            url = st.text_input(label='URLã‚’å…¥åŠ›ã—ã¦ãã ã•ã„',
                                value='https://www.asplund-contract.com/product/br/workplus/',
                                placeholder='https://example.com',
                                help='ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã—ãŸã„ãƒšãƒ¼ã‚¸ã®URLã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚')
        else:
            url = ''

        # URLè§£æãƒœã‚¿ãƒ³
        url_analyze_flag = st.button(label='URLè§£æ',
                                     icon='ğŸ”',
                                     use_container_width=True,
                                     disabled=True if url == '' else False)

    # URLä¸€è¦§ã®DataFrame
    df_urls = pd.DataFrame()

    # URLè§£æ
    if url_analyze_flag:
        if data_source_type == 0:  # XMLã‚µã‚¤ãƒˆãƒãƒƒãƒ—
            sm = SiteMapParser(url)  # reads /sitemap.xml
            json_exporter = JSONExporter(sm)
            if sm.has_urls():
                df_urls = pd.read_json(io.StringIO(json_exporter.export_urls()))
                df_urls.drop(columns=['lastmod', 'changefreq', 'priority'], inplace=True)
                df_urls.rename(columns={'loc': 'URL'}, inplace=True)
        elif data_source_type == 1:  # URL
            # asyncio.run(scrape_item_list(url))
            st.write("æœªå®Ÿè£…")

    else:
        df_urls = st.session_state['scraping_all_url_list']

    if len(df_urls) > 0:
        with st.expander(label='URLä¸€è¦§', expanded=True):
            st.session_state['scraping_all_url_list'] = df_urls
            selection = st.dataframe(df_urls,
                                     column_config={
                                         "URL": st.column_config.LinkColumn("URL")
                                     },
                                     use_container_width=True,
                                     hide_index=True,
                                     on_select="rerun",
                                     selection_mode="multi-row").selection
            # é¸æŠã•ã‚ŒãŸURLã‚’ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆã«ä¿å­˜
            if selection.rows:
                st.session_state['scraping_selected_url_list'] = df_urls.iloc[selection.rows, :]
            else:
                st.session_state['scraping_selected_url_list'] = pd.DataFrame()

            # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒœã‚¿ãƒ³
            if len(st.session_state['scraping_selected_url_list']) > 0:
                scraping_flag = st.button(label='ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°', icon='ğŸ•·ï¸', use_container_width=True)

    # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
    if scraping_flag:
        with st.expander(label='ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°', expanded=True):
            scraped_data = asyncio.run(
                scrape_data(st.session_state['scraping_selected_url_list']['URL'].tolist()))
            st.text_area(label='ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°çµæœ', value=scraped_data, height=300, disabled=True)
            st.button(label='ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜', icon='ğŸ’¾', use_container_width=True)


async def scrape_data(urls):

    class ProductInfo(BaseModel):
        brand_name: str = Field(..., description="ãƒ–ãƒ©ãƒ³ãƒ‰åã‚’è¡¨ã™æ–‡å­—åˆ—")
        item_name: str = Field(..., description="ã‚¢ã‚¤ãƒ†ãƒ åã‚’è¡¨ã™æ–‡å­—åˆ—")
        size: str = Field(..., description="ã‚µã‚¤ã‚ºã‚’è¡¨ã™æ–‡å­—åˆ—. ä¾‹ï¼šW500 D500 H550 ")
        weight: str = Field(..., description="é‡é‡ã‚’è¡¨ã™æ–‡å­—åˆ—. ä¾‹ï¼š15kg")
        material: str = Field(..., description="ç´ æã‚’è¡¨ã™æ–‡å­—åˆ—. ä¾‹ï¼šFabric, Steel")
        price: str = Field(..., description="ä¾¡æ ¼ã‚’è¡¨ã™æ–‡å­—åˆ—. ä¾‹ï¼šï¿¥120,000ï¼ˆç¨è¾¼ï¼‰")
        description: str = Field(...,
                                 description="å•†å“èª¬æ˜ã‚’è¡¨ã™æ–‡å­—åˆ—. ä¾‹ï¼šã“ã®ã‚½ãƒ•ã‚¡ã¯ã€å¿«é©ãªåº§ã‚Šå¿ƒåœ°ã¨ã‚¹ã‚¿ã‚¤ãƒªãƒƒã‚·ãƒ¥ãªãƒ‡ã‚¶ã‚¤ãƒ³ã‚’å…¼ã­å‚™ãˆã¦ã„ã¾ã™ã€‚")
        image_urls: list[str] = Field(..., description="å•†å“ç”»åƒã®URLãƒªã‚¹ãƒˆ")  # ç”»åƒã®URLãƒªã‚¹ãƒˆ

    browser_config = BrowserConfig(
        user_agent_mode="random",  # ãƒœãƒƒãƒˆæ¤œå‡ºã«å¯¾æŠ—
        verbose=True,
        # headless=False
    )
    # Define the LLM extraction strategy
    llm_strategy = LLMExtractionStrategy(
        llm_config=LLMConfig(provider="gemini/gemini-2.5-flash",
                             api_token=st.secrets['GEMINI_API_TOKEN']),
        schema=ProductInfo.model_json_schema(),
        extraction_type="schema",
        # extraction_type="block",
        verbose=True,
        instruction='''
ä¸ãˆã‚‰ã‚ŒãŸã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ã€ä»¥ä¸‹ã®JSONå½¢å¼ã«å¤‰æ›ã—ã¦å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚\n\n
ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚³ãƒ¼ãƒ‰ã§ã¯ãªãã€JSONå½¢å¼ã®ãƒ‡ãƒ¼ã‚¿ã‚’å‡ºåŠ›ã—ã¾ã™ã€‚
JSONä»¥å¤–ã®æ–‡å­—ã‚„ã‚³ãƒ¡ãƒ³ãƒˆã¯çµ¶å¯¾ã«çµ¶å¯¾ã«çµ¶å¯¾ã«å‡ºåŠ›ã—ãªã„ã§ãã ã•ã„ã€‚

# ä»¥ä¸‹ã®æ¡ä»¶ã‚’å¿…ãšå®ˆã£ã¦ãã ã•ã„:
- çµæœã¯å³å¯†ãªJSONã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’è¿”ã™ã“ã¨
- JSONä»¥å¤–ã®æ–‡å­—ã‚„ã‚³ãƒ¡ãƒ³ãƒˆã¯çµ¶å¯¾ã«çµ¶å¯¾ã«çµ¶å¯¾ã«å‡ºåŠ›ã—ãªã„
# å‡ºåŠ›ä¾‹
{
  "brand_name": "ã“ã“ã«ãƒ–ãƒ©ãƒ³ãƒ‰å",
  "item_name": "ã“ã“ã«å•†å“åã‚’å‡ºåŠ›",
  "size": "ã“ã“ã«ã‚µã‚¤ã‚ºã‚’mmå˜ä½ã«å¤‰æ›ã—ã¦å‡ºåŠ›",
  "weight": "ã“ã“ã«é‡é‡ã‚’kgå˜ä½ã«å¤‰æ›ã—ã¦å‡ºåŠ›",
  "material": "ã“ã“ã«ç´ æã‚’å‡ºåŠ›",
  "price": "ã“ã“ã«ä¾¡æ ¼ã‚’æ—¥æœ¬å††ã§å‡ºåŠ›ï¼ˆç¨è¾¼/ç¨æŠœãŒã‚ã‹ã‚‹ã‚ˆã†ã«è¨˜è¼‰ï¼‰",
  "description": "ã“ã“ã«å•†å“èª¬æ˜ã‚’å‡ºåŠ›",
  "image_urls": [
    "https://example.com/images/item1.jpg",
    "https://example.com/images/item2.jpg",
    "https://example.com/images/item3.jpg"
  ]
}
''',
        chunk_token_threshold=65536,
        overlap_rate=0.0,
        apply_chunking=True,
        input_format="markdown",  # or "html", "fit_markdown"
        extra_args={
            "temperature": 0.0,
            "max_tokens": 65536
        })
    run_config = CrawlerRunConfig(
        excluded_tags=['header', 'footer', 'nav'],  # é™¤å¤–ã™ã‚‹ã‚¿ã‚°
        extraction_strategy=llm_strategy,
        # word_count_threshold=10,  # Minimum words per content block. ã‚µã‚¤ãƒˆã«çŸ­ã„æ®µè½ã‚„é …ç›®ãŒå¤šæ•°ã‚ã‚‹å ´åˆã€ãƒ–ãƒ­ãƒƒã‚¯ãŒè€ƒæ…®ã•ã‚Œã‚‹å‰ã®æœ€å°å˜èªæ•°ã‚’ä¸‹ã’ã‚‹
        exclude_external_links=True,  # Remove external links
        exclude_social_media_links=True,  # Remove social media links
        exclude_external_images=True,  # Remove external images
        remove_overlay_elements=True,  # Remove popups/modals
        process_iframes=True,  # Process iframe content
        verbose=True)

    scraped_data = ''
    for url in urls:
        async with AsyncWebCrawler(config=browser_config) as crawler:
            result = await crawler.arun(url=url, config=run_config)
            if result.success:
                # st.write("markdown")
                # st.write(result.markdown)
                # LLMExtractionStrategyã‚’ä½¿ç”¨ã—ã¦ã„ã‚‹å ´åˆã€result.markdownã«æŠ½å‡ºã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ãŒå«ã¾ã‚Œã‚‹
                st.write(f"LLM: \n\n{result.extracted_content}")
                st.write(f"markdown: \n\n{result.markdown}")
                scraped_data += f"{result.markdown}\n\n"
                # st.write("media")
                # for image in result.media["images"]:
                #     st.image(image['src'], caption=image['alt'])
                # st.write("links")
                # for link in result.links["internal"]:
                #     st.markdown(f"[{link['text']}]({link['href']})")
            else:
                st.error(
                    f"Failed to scrape {url}: status_code:{result.status_code} message:{result.error_message}"
                )
                continue

    return scraped_data


async def scrape_item_list(url):
    browser_config = BrowserConfig(
        user_agent_mode="random",  # ãƒœãƒƒãƒˆæ¤œå‡ºã«å¯¾æŠ—
        verbose=True,
        # headless=False
    )
    run_config = CrawlerRunConfig(
        excluded_tags=['header', 'footer', 'nav'],  # é™¤å¤–ã™ã‚‹ã‚¿ã‚°
        # word_count_threshold=10,  # Minimum words per content block. ã‚µã‚¤ãƒˆã«çŸ­ã„æ®µè½ã‚„é …ç›®ãŒå¤šæ•°ã‚ã‚‹å ´åˆã€ãƒ–ãƒ­ãƒƒã‚¯ãŒè€ƒæ…®ã•ã‚Œã‚‹å‰ã®æœ€å°å˜èªæ•°ã‚’ä¸‹ã’ã‚‹
        exclude_external_links=True,  # Remove external links
        exclude_social_media_links=True,  # Remove social media links
        exclude_external_images=True,  # Remove external images
        remove_overlay_elements=True,  # Remove popups/modals
        process_iframes=True  # Process iframe content
    )

    async with AsyncWebCrawler(config=browser_config) as crawler:
        result = await crawler.arun(url=url, config=run_config)
        if result.success:
            st.write("markdown")
            st.write(result.markdown)
            # st.write("media")
            # for image in result.media["images"]:
            #     st.image(image['src'], caption=image['alt'])
            # st.write("links")
            # for link in result.links["internal"]:
            #     st.markdown(f"[{link['text']}]({link['href']})")
        else:
            st.error(
                f"Failed to scrape {url}: status_code:{result.status_code} message:{result.error_message}"
            )


async def deep_crawl_test(url):
    config = CrawlerRunConfig(deep_crawl_strategy=BFSDeepCrawlStrategy(max_depth=0,
                                                                       include_external=False),
                              scraping_strategy=LXMLWebScrapingStrategy(),
                              verbose=True)

    async with AsyncWebCrawler() as crawler:
        results = await crawler.arun(url, config=config)

        st.write(f"Crawled {len(results)} pages in total")

        # Access individual results
        for result in results[:3]:  # Show first 3 results
            st.write(f"URL: {result.url}")
            st.write(f"Depth: {result.metadata.get('depth', 0)}")
            st.write(result.markdown)  # Show first 500 characters of markdown


if __name__ == "__main__":
    # åˆæœŸåŒ–
    init()
    # ã‚µã‚¤ãƒ‰ãƒãƒ¼ã®è¡¨ç¤º
    menu = sidebar()
    # ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®è¡¨ç¤º
    if menu == 'å®¶å…·é¸å®š':
        # é¸å®šæ¡ä»¶ã®è¡¨ç¤º
        search_conditions()
        #
        if st.button(label='ä¿å­˜ã—ãŸæ¡ä»¶ã§æ¤œç´¢',
                     icon='ğŸ”',
                     disabled=len(st.session_state['furniture_condition_list']) == 0,
                     use_container_width=True):
            with st.chat_message(name="assistant"):
                st.markdown('''
    ã“ã¡ã‚‰ã®å•†å“ã¯ã„ã‹ãŒã§ã—ã‚‡ã†ã‹ï¼Ÿ  
    å•†å“åï¼š[PENTE 1P SOFA](https://www.asplund-contract.com/product/12426/)  
    ãƒ–ãƒ©ãƒ³ãƒ‰ï¼šWork Plus  
    Size:W760 D760 H670 SH425  
    Material:Fabric, Steel  
    Price:ï¿¥120,000  
    ![PENTE 1P SOFA](https://www.asplund-contract.com/wp-content/uploads/2024/06/wp_pente1psofa-3-600x600.jpg)
    
    
    å•†å“åï¼š[MELTONE 1P SOFA](https://www.asplund-contract.com/product/12396/)  
    ãƒ–ãƒ©ãƒ³ãƒ‰ï¼šWork Plus  
    Size:W870 D720 H750 SH410
    Material:Fabric, Steel  
    Price:ï¿¥110,000  
    ![PENTE 1P SOFA](https://www.asplund-contract.com/wp-content/uploads/2024/06/wp_meltone1psofa-1-600x600.jpg)
    ''')

        # ãƒãƒ£ãƒƒãƒˆå…¥åŠ›
        chat_input()
    elif menu == 'ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°':
        scrapintg()
    elif menu == 'PDFãƒ‡ãƒ¼ã‚¿æŠ½å‡º':
        st.write("æœªå®Ÿè£…")
    else:
        st.write("")

    # with st.expander(label='session', expanded=False):
    #     st.write(st.session_state)
